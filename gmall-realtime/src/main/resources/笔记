1.Dim层将维度表数据写入到hbase，通过phoenix,通过读取的mysql表配置信息，广播出去与主流进行connect，然后通过process算子判断到来的数据是不是属于维度表的信息，
并且实现在process算子中自动建表

2.DWD层关联的字典表是保存在mysql中的，通过JDBC SQL Connector 建立的mysql表，"primary key(`dic_code`) not enforced " +   //注意：not enforced 表示不对主键做强制的唯一性约束和非空约束校验，
Flink 并不是数据的主人，因此只支持 not enforced 模式

3.Flink的AsyncDataStream是一个异步的数据流操作，它允许异步的处理流数据，可以在数据到达之前并行地访问外部系统和服务，从而提高整个应用程序的处理性能和吞吐量。
AsyncDataStream可以接受一个DataStream作为输入，通过调用一个异步函数来异步处理每个数据元素，并将结果作为一个Future对象发送到下游。当结果就绪时，
会通过回调函数将结果推送到下游操作。AsyncDataStream需要指定异步函数、异步函数执行的最大并发数以及异步函数的超时时间。

4.JdbcSink.<T>sink(sql,new JdbcStatementBuilder<T>() {可以使用反射获取类中的属性，并且根据@TransientSink注解来判断该属性是否要写入到sql占位符中，主要用于数据写出到phoenix

5.Upsert Kafka Connector支持以 upsert 方式从 Kafka topic 中读写数据，Kafka Connector支持从 Kafka topic 中读写数据
  Kafka Connector 要求表不能有主键,Upsert Kafka Connector 要求表必须有主键,Kafka Connector 不能消费带有 Upsert/Delete 操作类型数据的表，如 left join 生成的动态表
  由于 left join 的存在，流中存在修改数据，所以写出数据使用 Upsert Kafka Connector 详看类DwdTradeOrderPreProcess


6.流如何跟hbase 维表关联，可以通过Flink的AsyncDataStream方法，在异步调用的时候，获取维表的数据，第一次读取可以选择将读取出来的维表数据储存到redis中，避免重复读取

7.MyKafkaUtil类中，当Kafka消费者从Kafka消息中读取数据时，KafkaDeserializationSchema的实现将负责将序列化的数据反序列化为Java对象。
isEndOfStream方法用于检查是否到达Kafka消息流的末尾。该方法的返回值为布尔值，如果当前Kafka消息已经到达流的末尾，则返回true，否则返回false
TypeInformation方法允许KafkaDeserializationSchema指定反序列化后的Java对象的类型，以便Flink运行时能够了解数据的类型信息

8. json与string互转，json与对象互转
  JSONObject jsonObject = JSON.parseObject(value);  String type = jsonObject.getString("type");
  TableProcess tableProcess = JSON.parseObject(jsonObject.getString("after"), TableProcess.class);

9. 多维json数组的使用
  JSONArray displays = value.getJSONArray("displays");
  JSONObject display = displays.getJSONObject(i);
  display.put("common", common);
  display.toJSONString()

10.String a = "{\"name\":\"zs\",\"age\":14}";
   String b = "name";
   JSONObject jsonObject = JSON.parseObject(a);
   Set<Map.Entry<String, Object>> entries = jsonObject.entrySet();
   entries.removeIf(next -> !b.contains(next.getKey()));
   System.out.println(jsonObject);
   结果：{"name":"zs"} 这个可以去除json里面的属性

11.若读取的json里面有多个嵌套字段，在构建kafka 表时，可声明为MAP<>()格式如：参考详看类DwdTradeOrderPreProcess
{"database":"gmall","table":"cart_info","type":"update","ts":1592270938,"xid":13090,"xoffset":1573,"data":
{"id":100924,"user_id":"93","sku_id":16,"cart_price":4488.00,"sku_num":1,"img_url":"http://47.93.148.192:8080/group1/M00/00/02/rBHu8l-sklaALrngAAHGDqdpFtU741.jpg",
"sku_name":"华为 HUAWEI P40 麒麟990 5G SoC芯片 5000万超感知徕卡三摄 30倍数字变焦 8GB+128GB亮黑色全网通5G手机","is_checked":null,
"create_time":"2020-06-14 09:28:57","operate_time":null,"is_ordered":1,"order_time":"2021-10-17 09:28:58","source_type":"2401","source_id":null},
"old":{"is_ordered":0,"order_time":null}}
创建方式为： return "CREATE TABLE topic_db ( " +
                      "  `database` STRING, " +
                      "  `table` STRING, " +
                      "  `type` STRING, " +
                      "  `data` MAP<STRING,STRING>, " +
                      "  `old` MAP<STRING,STRING>, " +
                      "  `pt` AS PROCTIME() " +
调用为：
  "select " +
                "    data['id'] id, " +
                "    data['order_id'] order_id, " +
                "    data['sku_id'] sku_id, " +
                "    data['sku_name'] sku_name, " +
                "    data['order_price'] order_price, " +
                "    data['sku_num'] sku_num, " +
                "    data['create_time'] create_time, " +
                "    data['source_type'] source_type, " +
                "    data['source_id'] source_id, " +
                "    data['split_total_amount'] split_total_amount, " +
                "    data['split_activity_amount'] split_activity_amount, " +
                "    data['split_coupon_amount'] split_coupon_amount, " +
                "    pt  " +
                "from topic_db " +
                "where `database` = 'gmall-211126-flink' "

12.所有Flink函数类都有其Rich版本。它与常规函数的不同在于，可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。
也有意味着提供了更多的，更丰富的功能，比如richmapFunction,生命周期方法有open,close,getRuntimeContext()方法提供了函数的RuntimeContext的一些信息，
例如函数执行的并行度，任务的名字，以及state状态. 开发人员在需要的时候自行调用获取运行时上下文对象

13.以下用在api中
intervalJoin：段 join，两条流的每一条数据都可以与另一条流某个时间范围内的数据做关联。底层实现原理：以 A.intervalJoin(B) 为例，
A 流中的数据进入算子后，会被保存到键控状态中，同时注册一个定时器，定时器触发时清空 A 流状态中的数据。在定时器触发之前，B 流中的每一条数据都可以与状态中保存的 A 流数据关联。
同理，B 流中也维护了状态定时器。由此实现了段 join。假定A流中的定时器存在时长为3s，B流中的定时器存在时长为5s，A 流中某条数据抵达时间为 tA，
可与 tA – 5s ~ tA + 3s 时间范围内抵达的 B 流数据关联；B 流中某条数据抵达时间为 tB，可与 tB – 3s ~ tB + 5s 时间范围内抵达的 A 流数据关联。
	union()：用于两条及多条流之间的合并，对流的数量没有限制，但是要求所有流中的数据结构完全一致。详见DwsTrafficVcChArIsNewPageViewWindow
	connect()：用于两条流的合并，其后紧邻的 process 算子中可以使用的 CoProcessFunction 是双流处理最底层的 API，可以通过键控状态和定时器的运用实现join、广播join、
段join等各种关联。connect() 只能对两条流做关联，且对两条流的数据结构没有要求。
	join()：该算子的功能可以被其它算子替代，目前基本不用。

13.ProcessFunction API(底层API)
我们之前学习的转换算子是无法访问事件的时间戳信息和水位线信息的。而这在一些应用场景下，极为重要。例如MapFunction这样的map转换算子就无法访问时间戳或者当前事件的事件时间。
基于此，DataStream API提供了一系列的Low-Level转换算子。可以访问时间戳、watermark以及注册定时事件。还可以输出特定的一些事件，例如超时事件等。
Process Function用来构建事件驱动的应用以及实现自定义的业务逻辑(使用之前的window函数和转换算子无法实现)。例如，Flink SQL就是使用Process Function实现的
env
  .socketTextStream("hadoop102", 9999)
  .map(line -> {
      String[] datas = line.split(",");
      return new WaterSensor(datas[0], Long.valueOf(datas[1]), Integer.valueOf(datas[2]));
  })
  .process(new ProcessFunction<WaterSensor, String>() {
      @Override
      public void processElement(WaterSensor value, Context ctx, Collector<String> out) throws Exception {
          out.collect(value.toString());
      }
  })
  .print();
Context和OnTimerContext所持有的TimerService对象拥有以下方法:
	currentProcessingTime(): Long 返回当前处理时间
	currentWatermark(): Long 返回当前watermark的时间戳
	registerProcessingTimeTimer(timestamp: Long): Unit 会注册当前key的processing time的定时器。当processing time到达定时时间时，触发timer。
	registerEventTimeTimer(timestamp: Long): Unit 会注册当前key的event time 定时器。当水位线大于等于定时器注册的时间时，触发定时器执行回调函数。
	deleteProcessingTimeTimer(timestamp: Long): Unit 删除之前注册处理时间定时器。如果没有这个时间戳的定时器，则不执行。
	deleteEventTimeTimer(timestamp: Long): Unit 删除之前注册的事件时间定时器，如果没有此时间戳的定时器，则不执行

14.字符串拼接sql
String sql = "upsert into " + GmallConfig.HBASE_SCHEMA + "." + sinkTable + "(" +
                   StringUtils.join(columns, ",") + ") values ('" +
                   StringUtils.join(values, "','") + "')";

15.https://mp.weixin.qq.com/s/L20Nl5YrCpIMFbL9OHq7Sw flink 1.11 中的随机数据生成器-DataGen connector