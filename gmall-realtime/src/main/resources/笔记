1.Dim层将维度表数据写入到hbase，通过phoenix,通过读取的mysql表配置信息，广播出去与主流进行connect，然后通过process算子判断到来的数据是不是属于维度表的信息，
并且实现在process算子中自动建表

2.DWD层关联的字典表是保存在mysql中的，通过JDBC SQL Connector 建立的mysql表，"primary key(`dic_code`) not enforced " +   //注意：not enforced 表示不对主键做强制的唯一性约束和非空约束校验，
Flink 并不是数据的主人，因此只支持 not enforced 模式

3.Flink的AsyncDataStream是一个异步的数据流操作，它允许异步的处理流数据，可以在数据到达之前并行地访问外部系统和服务，从而提高整个应用程序的处理性能和吞吐量。
AsyncDataStream可以接受一个DataStream作为输入，通过调用一个异步函数来异步处理每个数据元素，并将结果作为一个Future对象发送到下游。当结果就绪时，
会通过回调函数将结果推送到下游操作。AsyncDataStream需要指定异步函数、异步函数执行的最大并发数以及异步函数的超时时间。

4.JdbcSink.<T>sink(sql,new JdbcStatementBuilder<T>() {可以使用反射获取类中的属性，并且根据@TransientSink注解来判断该属性是否要写入到sql占位符中，主要用于数据写出到phoenix

5.Upsert Kafka Connector支持以 upsert 方式从 Kafka topic 中读写数据，Kafka Connector支持从 Kafka topic 中读写数据
  Kafka Connector 要求表不能有主键,Upsert Kafka Connector 要求表必须有主键,Kafka Connector 不能消费带有 Upsert/Delete 操作类型数据的表，如 left join 生成的动态表
  由于 left join 的存在，流中存在修改数据，所以写出数据使用 Upsert Kafka Connector


6.流如何跟hbase 维表关联，可以通过Flink的AsyncDataStream方法，在异步调用的时候，获取维表的数据，第一次读取可以选择将读取出来的维表数据储存到redis中，避免重复读取

7.MyKafkaUtil类中，当Kafka消费者从Kafka消息中读取数据时，KafkaDeserializationSchema的实现将负责将序列化的数据反序列化为Java对象。
isEndOfStream方法用于检查是否到达Kafka消息流的末尾。该方法的返回值为布尔值，如果当前Kafka消息已经到达流的末尾，则返回true，否则返回false
TypeInformation方法允许KafkaDeserializationSchema指定反序列化后的Java对象的类型，以便Flink运行时能够了解数据的类型信息

8. json与string互转，json与对象互转
  JSONObject jsonObject = JSON.parseObject(value);  String type = jsonObject.getString("type");
  TableProcess tableProcess = JSON.parseObject(jsonObject.getString("after"), TableProcess.class);

9. 多维json数组的使用
  JSONArray displays = value.getJSONArray("displays");
  JSONObject display = displays.getJSONObject(i);
  display.put("common", common);
  display.toJSONString()

10.String a = "{\"name\":\"zs\",\"age\":14}";
   String b = "name";
   JSONObject jsonObject = JSON.parseObject(a);
   Set<Map.Entry<String, Object>> entries = jsonObject.entrySet();
   entries.removeIf(next -> !b.contains(next.getKey()));
   System.out.println(jsonObject);
   结果：{"name":"zs"} 这个可以去除json里面的属性