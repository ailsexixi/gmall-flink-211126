1.Dim层将维度表数据写入到hbase，通过phoenix,通过读取的mysql表配置信息，广播出去与主流进行connect，然后通过process算子判断到来的数据是不是属于维度表的信息，
并且实现在process算子中自动建表

2.DWD层关联的字典表是保存在mysql中的，通过JDBC SQL Connector 建立的mysql表，"primary key(`dic_code`) not enforced " +   //注意：not enforced 表示不对主键做强制的唯一性约束和非空约束校验，
Flink 并不是数据的主人，因此只支持 not enforced 模式

3.Flink的AsyncDataStream是一个异步的数据流操作，它允许异步的处理流数据，可以在数据到达之前并行地访问外部系统和服务，从而提高整个应用程序的处理性能和吞吐量。
AsyncDataStream可以接受一个DataStream作为输入，通过调用一个异步函数来异步处理每个数据元素，并将结果作为一个Future对象发送到下游。当结果就绪时，
会通过回调函数将结果推送到下游操作。AsyncDataStream需要指定异步函数、异步函数执行的最大并发数以及异步函数的超时时间。

4.JdbcSink.<T>sink(sql,new JdbcStatementBuilder<T>() {可以使用反射获取类中的属性，并且根据@TransientSink注解来判断该属性是否要写入到sql占位符中，主要用于数据写出到phoenix

5.Upsert Kafka Connector支持以 upsert 方式从 Kafka topic 中读写数据，Kafka Connector支持从 Kafka topic 中读写数据
  Kafka Connector 要求表不能有主键,Upsert Kafka Connector 要求表必须有主键,Kafka Connector 不能消费带有 Upsert/Delete 操作类型数据的表，如 left join 生成的动态表
  由于 left join 的存在，流中存在修改数据，所以写出数据使用 Upsert Kafka Connector 详看类DwdTradeOrderPreProcess


6.流如何跟hbase 维表关联，可以通过Flink的AsyncDataStream方法，在异步调用的时候，获取维表的数据，第一次读取可以选择将读取出来的维表数据储存到redis中，避免重复读取

7.MyKafkaUtil类中，当Kafka消费者从Kafka消息中读取数据时，KafkaDeserializationSchema的实现将负责将序列化的数据反序列化为Java对象。
isEndOfStream方法用于检查是否到达Kafka消息流的末尾。该方法的返回值为布尔值，如果当前Kafka消息已经到达流的末尾，则返回true，否则返回false
TypeInformation方法允许KafkaDeserializationSchema指定反序列化后的Java对象的类型，以便Flink运行时能够了解数据的类型信息

8. json与string互转，json与对象互转
  JSONObject jsonObject = JSON.parseObject(value);  String type = jsonObject.getString("type");
  TableProcess tableProcess = JSON.parseObject(jsonObject.getString("after"), TableProcess.class);

9. 多维json数组的使用
  JSONArray displays = value.getJSONArray("displays");
  JSONObject display = displays.getJSONObject(i);
  display.put("common", common);
  display.toJSONString()

10.String a = "{\"name\":\"zs\",\"age\":14}";
   String b = "name";
   JSONObject jsonObject = JSON.parseObject(a);
   Set<Map.Entry<String, Object>> entries = jsonObject.entrySet();
   entries.removeIf(next -> !b.contains(next.getKey()));
   System.out.println(jsonObject);
   结果：{"name":"zs"} 这个可以去除json里面的属性

11.若读取的json里面有多个嵌套字段，在构建kafka 表时，可声明为MAP<>()格式如：参考详看类DwdTradeOrderPreProcess
{"database":"gmall","table":"cart_info","type":"update","ts":1592270938,"xid":13090,"xoffset":1573,"data":
{"id":100924,"user_id":"93","sku_id":16,"cart_price":4488.00,"sku_num":1,"img_url":"http://47.93.148.192:8080/group1/M00/00/02/rBHu8l-sklaALrngAAHGDqdpFtU741.jpg",
"sku_name":"华为 HUAWEI P40 麒麟990 5G SoC芯片 5000万超感知徕卡三摄 30倍数字变焦 8GB+128GB亮黑色全网通5G手机","is_checked":null,
"create_time":"2020-06-14 09:28:57","operate_time":null,"is_ordered":1,"order_time":"2021-10-17 09:28:58","source_type":"2401","source_id":null},
"old":{"is_ordered":0,"order_time":null}}
创建方式为： return "CREATE TABLE topic_db ( " +
                      "  `database` STRING, " +
                      "  `table` STRING, " +
                      "  `type` STRING, " +
                      "  `data` MAP<STRING,STRING>, " +
                      "  `old` MAP<STRING,STRING>, " +
                      "  `pt` AS PROCTIME() " +
调用为：
  "select " +
                "    data['id'] id, " +
                "    data['order_id'] order_id, " +
                "    data['sku_id'] sku_id, " +
                "    data['sku_name'] sku_name, " +
                "    data['order_price'] order_price, " +
                "    data['sku_num'] sku_num, " +
                "    data['create_time'] create_time, " +
                "    data['source_type'] source_type, " +
                "    data['source_id'] source_id, " +
                "    data['split_total_amount'] split_total_amount, " +
                "    data['split_activity_amount'] split_activity_amount, " +
                "    data['split_coupon_amount'] split_coupon_amount, " +
                "    pt  " +
                "from topic_db " +
                "where `database` = 'gmall-211126-flink' "

12.所有Flink函数类都有其Rich版本。它与常规函数的不同在于，可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。
也有意味着提供了更多的，更丰富的功能，比如richmapFunction,生命周期方法有open,close,getRuntimeContext()方法提供了函数的RuntimeContext的一些信息，
例如函数执行的并行度，任务的名字，以及state状态. 开发人员在需要的时候自行调用获取运行时上下文对象